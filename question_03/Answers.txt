3a. I divided the overall dataset into train, test and validation in 80, 10, 10 split. The validation data is used during the training stage 
to choose the best model among the trained models. I created these sets with almost uniform sampling across the real and fake sets as explained in Question1.
3b. I have not considered any data augmentation methods due time constraints. But some possible methods would be cropping, rotations, translations.
Additionally some sort of batch normalization in the model can account for internal covariance shift between the train and test sets. 
3c. Attached output with 0.5 threshold. This model gave an accuracy of 56% on test set and AUROC of 0.6